name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  # Detect which paths changed to conditionally run jobs
  changes:
    # Skip release commits - they only update version/changelog, already tested via PR
    if: "!startsWith(github.event.head_commit.message, 'chore(release):')"
    runs-on: ubuntu-latest
    outputs:
      src: ${{ steps.filter.outputs.src }}
      tests: ${{ steps.filter.outputs.tests }}
      docs: ${{ steps.filter.outputs.docs }}
      deps: ${{ steps.filter.outputs.deps }}
    steps:
      - uses: actions/checkout@v6
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            src:
              - 'src/**'
            tests:
              - 'tests/**'
            docs:
              - 'docs/**'
              - '*.md'
              - '.readthedocs.yaml'
            deps:
              - 'pyproject.toml'

  pre-commit:
    if: "!startsWith(github.event.head_commit.message, 'chore(release):')"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v6
        with:
          python-version: "3.12"
      - uses: pre-commit/action@v3.0.1

  test:
    needs: [changes, pre-commit]
    # Only run tests when source code, tests, or dependencies change
    if: |
      needs.changes.outputs.src == 'true' ||
      needs.changes.outputs.tests == 'true' ||
      needs.changes.outputs.deps == 'true'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: true
      matrix:
        include:
          # Spark 3.5.x with Python 3.11 and 3.12 (NumPy 1.x)
          - python-version: "3.11"
            spark-version: "3.5"
            pyspark: "pyspark>=3.5.0,<4.0.0"
            numpy: "numpy>=1.24.0,<2.0.0"
            pandas: "pandas>=1.5.0,<3.0.0"
            pyarrow: "pyarrow>=12.0.0,<17.0.0"
          - python-version: "3.12"
            spark-version: "3.5"
            pyspark: "pyspark>=3.5.0,<4.0.0"
            numpy: "numpy>=1.26.0,<2.0.0"
            pandas: "pandas>=2.0.0,<3.0.0"
            pyarrow: "pyarrow>=14.0.0,<17.0.0"
            extra-deps: "setuptools"  # Provides distutils for PySpark 3.5 on Python 3.12+
          # Spark 4.x with Python 3.12 and 3.13 (NumPy 2.x)
          - python-version: "3.12"
            spark-version: "4.x"
            pyspark: "pyspark>=4.0.0,<5.0.0"
            numpy: "numpy>=2.0.0,<3.0.0"
            pandas: "pandas>=2.2.0,<3.0.0"
            pyarrow: "pyarrow>=17.0.0,<19.0.0"
          - python-version: "3.13"
            spark-version: "4.x"
            pyspark: "pyspark>=4.0.0,<5.0.0"
            numpy: "numpy>=2.0.0,<3.0.0"
            pandas: "pandas>=2.2.0,<3.0.0"
            pyarrow: "pyarrow>=17.0.0,<19.0.0"
          # Ray backend testing (without PySpark to verify optional dependency)
          - python-version: "3.12"
            spark-version: "ray-only"
            numpy: "numpy>=2.0.0,<3.0.0"
            pandas: "pandas>=2.2.0,<3.0.0"
            pyarrow: "pyarrow>=17.0.0,<19.0.0"
            extra-deps: "ray[default]>=2.9.0,<3.0.0"
            ray: true

    name: Test (Python ${{ matrix.python-version }}, Spark ${{ matrix.spark-version }}${{ matrix.ray && ', Ray' || '' }})
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          pip install -e ".[test-base]"
          pip install "${{ matrix.numpy }}" "${{ matrix.pandas }}" "${{ matrix.pyarrow }}"
          if [ -n "${{ matrix.pyspark }}" ]; then pip install "${{ matrix.pyspark }}"; fi
          if [ -n "${{ matrix.extra-deps }}" ]; then pip install "${{ matrix.extra-deps }}"; fi
      - name: Show installed versions
        run: |
          python -c "import pyspark; print(f'PySpark: {pyspark.__version__}')" 2>/dev/null || echo "PySpark: not installed"
          python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
          python -c "import pandas; print(f'Pandas: {pandas.__version__}')"
          python -c "import pyarrow; print(f'PyArrow: {pyarrow.__version__}')"
          python -c "import ray; print(f'Ray: {ray.__version__}')" 2>/dev/null || echo "Ray: not installed"
      - name: Run tests
        run: |
          if [ "${{ matrix.ray }}" = "true" ]; then
            # Ray job: only run Ray-specific tests (faster, disable pytest-spark plugin)
            pytest tests/test_ray_backend.py -v -p no:spark
          else
            # Standard jobs: full test suite with coverage
            pytest --cov=src/spark_bestfit --cov-report=term-missing --cov-report=xml --cov-fail-under=75 -v
          fi
      - name: Upload coverage
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.12' && matrix.spark-version == '4.x' && !matrix.ray
        with:
          files: coverage.xml
          fail_ci_if_error: false

  security:
    name: Security Scan
    needs: [changes, pre-commit]
    runs-on: ubuntu-latest
    # Run when source or deps change, skip for release-please PRs
    if: |
      !startsWith(github.head_ref, 'release-please') && (
        needs.changes.outputs.src == 'true' ||
        needs.changes.outputs.deps == 'true'
      )
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v6
        with:
          python-version: "3.12"
      - name: Install dependencies
        run: |
          pip install pip-audit
          pip install -e ".[spark]"
      # pip-audit: Scan installed packages for known CVEs
      # Uses PyPI Advisory Database (https://github.com/pypa/advisory-database)
      - name: Security Scan (pip-audit)
        continue-on-error: true
        run: |
          pip-audit --format=json --output=pip-audit-report.json || true
          pip-audit --format=markdown --output=pip-audit-report.md || true
          pip-audit
      - name: Upload security report
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: security-report
          path: |
            pip-audit-report.json
            pip-audit-report.md
          retention-days: 30

  docs:
    needs: [changes, pre-commit]
    # Run when source, docs, or dependencies change
    if: |
      needs.changes.outputs.src == 'true' ||
      needs.changes.outputs.docs == 'true' ||
      needs.changes.outputs.deps == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v6
        with:
          python-version: "3.12"
      - name: Install dependencies
        run: pip install -e ".[docs]"
      - name: Build documentation
        run: sphinx-build -b html docs docs/_build/html -W --keep-going

  # Summary job for branch protection - single required check
  ci:
    needs: [changes, test, docs]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Check all jobs passed
        run: |
          # Get job results (skipped jobs are OK - they were intentionally filtered)
          test_result="${{ needs.test.result }}"
          docs_result="${{ needs.docs.result }}"

          echo "Job results:"
          echo "  test: $test_result"
          echo "  docs: $docs_result"

          # Check each job - 'success' or 'skipped' are acceptable
          for job in test docs; do
            result_var="${job}_result"
            result="${!result_var}"
            if [ "$result" != "success" ] && [ "$result" != "skipped" ]; then
              echo "Job '$job' failed with result: $result"
              exit 1
            fi
          done

          echo "All required jobs passed or were correctly skipped"
