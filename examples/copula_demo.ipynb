{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13be88-bf77-420b-9047-26d0f08f8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gaussian Copula Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Why Copulas Matter** - Preserving correlation structure\n",
    "2. **Multi-Column Fitting** - Fit distributions to multiple columns\n",
    "3. **Copula Fitting** - Capture correlation via Spark ML (scales to billions)\n",
    "4. **Correlated Sampling** - Local and distributed sampling\n",
    "5. **Serialization** - Save and load copulas\n",
    "6. **Performance Benchmark** - spark-bestfit vs statsmodels at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from spark_bestfit import DistributionFitter, GaussianCopula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .appName(\"CopulaDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Why Copulas Matter: The Correlation Problem\n",
    "\n",
    "When you fit distributions to columns independently, the correlation between columns is lost. Let's demonstrate this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data: price and quantity with negative correlation\n",
    "# (higher price -> lower quantity, like demand curves)\n",
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "\n",
    "# Create correlated normal samples\n",
    "correlation = -0.7  # Strong negative correlation\n",
    "cov_matrix = [[1.0, correlation], [correlation, 1.0]]\n",
    "correlated_normals = np.random.multivariate_normal([0, 0], cov_matrix, n_samples)\n",
    "\n",
    "# Transform to different distributions (price: lognormal, quantity: gamma)\n",
    "from scipy import stats as st\n",
    "price = st.lognorm.ppf(st.norm.cdf(correlated_normals[:, 0]), s=0.5, scale=100)\n",
    "quantity = st.gamma.ppf(st.norm.cdf(correlated_normals[:, 1]), a=5, scale=20)\n",
    "\n",
    "# Create DataFrame\n",
    "pdf = pd.DataFrame({\"price\": price, \"quantity\": quantity})\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Show the original correlation\n",
    "original_corr = pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "print(f\"Original Spearman correlation: {original_corr:.3f}\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(pdf.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. The Problem: Independent Sampling Loses Correlation\n",
    "\n",
    "If we fit each column independently and sample separately, correlation is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions independently\n",
    "fitter = DistributionFitter(spark, random_seed=42)\n",
    "results = fitter.fit(df, columns=[\"price\", \"quantity\"], max_distributions=10)\n",
    "\n",
    "# Get best fits\n",
    "best_price = results.for_column(\"price\").best(n=1)[0]\n",
    "best_quantity = results.for_column(\"quantity\").best(n=1)[0]\n",
    "\n",
    "print(f\"Best fit for price: {best_price.distribution}\")\n",
    "print(f\"Best fit for quantity: {best_quantity.distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample independently - correlation is LOST!\n",
    "independent_price = best_price.sample(size=10_000, random_state=42)\n",
    "independent_quantity = best_quantity.sample(size=10_000, random_state=42)\n",
    "\n",
    "independent_pdf = pd.DataFrame({\n",
    "    \"price\": independent_price,\n",
    "    \"quantity\": independent_quantity\n",
    "})\n",
    "\n",
    "independent_corr = independent_pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "print(f\"Original correlation:    {original_corr:.3f}\")\n",
    "print(f\"Independent correlation: {independent_corr:.3f}  <- LOST!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. The Solution: Gaussian Copula\n",
    "\n",
    "A Gaussian copula preserves both:\n",
    "- **Marginal distributions**: Each column follows its fitted distribution\n",
    "- **Correlation structure**: Columns maintain their original relationships\n",
    "\n",
    "**Big Data Advantage**: Unlike standard libraries that require `.toPandas()`, spark-bestfit computes correlation via Spark ML - scaling to billions of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the copula - correlation computed via Spark ML (no .toPandas()!)\n",
    "copula = GaussianCopula.fit(results, df)\n",
    "\n",
    "print(f\"Columns: {copula.column_names}\")\n",
    "print(f\"\\nCorrelation matrix (computed via Spark ML):\")\n",
    "print(pd.DataFrame(\n",
    "    copula.correlation_matrix,\n",
    "    index=copula.column_names,\n",
    "    columns=copula.column_names\n",
    ").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample with correlation preserved!\n",
    "copula_samples = copula.sample(n=10_000, random_state=42)\n",
    "copula_pdf = pd.DataFrame(copula_samples)\n",
    "\n",
    "copula_corr = copula_pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "print(f\"Original correlation:    {original_corr:.3f}\")\n",
    "print(f\"Independent correlation: {independent_corr:.3f}  <- Lost\")\n",
    "print(f\"Copula correlation:      {copula_corr:.3f}  <- Preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Distributed Sampling with `sample_spark()`\n",
    "\n",
    "For large-scale sampling (millions of correlated samples), use `sample_spark()` to leverage Spark's distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100,000 correlated samples using Spark\n",
    "samples_df = copula.sample_spark(n=100_000, random_seed=42)\n",
    "\n",
    "print(f\"Schema: {samples_df.schema}\")\n",
    "print(f\"\\nSample preview:\")\n",
    "samples_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correlation is preserved at scale\n",
    "spark_samples_pdf = samples_df.toPandas()\n",
    "spark_corr = spark_samples_pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "\n",
    "print(f\"Original correlation:     {original_corr:.3f}\")\n",
    "print(f\"Spark sample correlation: {spark_corr:.3f}\")\n",
    "print(f\"Difference:               {abs(original_corr - spark_corr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Verifying Marginal Distributions\n",
    "\n",
    "The copula preserves marginal distributions - each column still follows its fitted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-S test: verify samples match the fitted marginal distributions\n",
    "for col in copula.column_names:\n",
    "    marginal = copula.marginals[col]\n",
    "    dist = marginal.get_scipy_dist()\n",
    "    samples = copula_pdf[col].values\n",
    "    \n",
    "    # K-S test against fitted distribution\n",
    "    ks_stat, p_value = st.kstest(samples, lambda x: dist.cdf(x, *marginal.parameters))\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Distribution: {marginal.distribution}\")\n",
    "    print(f\"  K-S statistic: {ks_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4f} {'(good fit!)' if p_value > 0.05 else '(poor fit)'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Serialization: Save and Load Copulas\n",
    "\n",
    "Save fitted copulas for later use - great for production pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Save to JSON (human-readable, recommended)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    json_path = os.path.join(tmpdir, \"copula.json\")\n",
    "    copula.save(json_path)\n",
    "    \n",
    "    # Show the JSON contents\n",
    "    with open(json_path, \"r\") as f:\n",
    "        import json\n",
    "        data = json.load(f)\n",
    "        print(\"JSON structure:\")\n",
    "        print(f\"  schema_version: {data['schema_version']}\")\n",
    "        print(f\"  type: {data['type']}\")\n",
    "        print(f\"  columns: {data['column_names']}\")\n",
    "        print(f\"  marginals: {list(data['marginals'].keys())}\")\n",
    "    \n",
    "    # Load and verify\n",
    "    loaded = GaussianCopula.load(json_path)\n",
    "    loaded_samples = loaded.sample(n=1000, random_state=42)\n",
    "    print(f\"\\nLoaded copula works! Generated {len(loaded_samples['price'])} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Three-Column Example\n",
    "\n",
    "Copulas work with any number of columns (minimum 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data: price, quantity, revenue\n",
    "np.random.seed(123)\n",
    "n = 10_000\n",
    "\n",
    "# Create correlation structure\n",
    "corr_3d = [\n",
    "    [1.0,  -0.6, 0.3],   # price: neg corr with quantity, pos with revenue\n",
    "    [-0.6, 1.0,  0.5],   # quantity: neg corr with price, pos with revenue\n",
    "    [0.3,  0.5,  1.0],   # revenue: pos corr with both\n",
    "]\n",
    "correlated_normals_3d = np.random.multivariate_normal([0, 0, 0], corr_3d, n)\n",
    "\n",
    "# Transform to different distributions\n",
    "price_3d = st.lognorm.ppf(st.norm.cdf(correlated_normals_3d[:, 0]), s=0.3, scale=50)\n",
    "quantity_3d = st.gamma.ppf(st.norm.cdf(correlated_normals_3d[:, 1]), a=10, scale=5)\n",
    "revenue_3d = st.norm.ppf(st.norm.cdf(correlated_normals_3d[:, 2]), loc=1000, scale=200)\n",
    "\n",
    "pdf_3d = pd.DataFrame({\n",
    "    \"price\": price_3d,\n",
    "    \"quantity\": quantity_3d,\n",
    "    \"revenue\": revenue_3d\n",
    "})\n",
    "df_3d = spark.createDataFrame(pdf_3d)\n",
    "\n",
    "print(\"Original correlation matrix:\")\n",
    "print(pdf_3d.corr(method=\"spearman\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions and copula\n",
    "results_3d = fitter.fit(df_3d, columns=[\"price\", \"quantity\", \"revenue\"], max_distributions=10)\n",
    "copula_3d = GaussianCopula.fit(results_3d, df_3d)\n",
    "\n",
    "# Sample and compare correlations\n",
    "samples_3d = copula_3d.sample(n=10_000, random_state=42)\n",
    "samples_3d_pdf = pd.DataFrame(samples_3d)\n",
    "\n",
    "print(\"Copula correlation matrix (should match original):\")\n",
    "print(samples_3d_pdf.corr(method=\"spearman\").round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i9z4an1lm7",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmark: When to Use spark-bestfit\n",
    "\n",
    "**Important context**: spark-bestfit is NOT faster than statsmodels for small data on a single machine. The value is **scale** - handling data that doesn't fit in memory and generating samples across a cluster.\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Data fits in memory (< 10M rows) | statsmodels is faster |\n",
    "| Data exceeds memory (100M+ rows) | spark-bestfit (only option) |\n",
    "| Data already in Spark | spark-bestfit (avoid `.toPandas()`) |\n",
    "| Need 100M+ samples | `sample_spark()` (distributed) |\n",
    "\n",
    "> **Note**: This section requires `statsmodels` for comparison: `pip install statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ud5fu34se",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from statsmodels.distributions.copula.api import GaussianCopula as StatsmodelsGaussianCopula\n",
    "from scipy.stats import norm, gamma, lognorm\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "def benchmark_correlation_only(n_rows: int, n_runs: int = 3):\n",
    "    \"\"\"Benchmark ONLY correlation computation: pandas vs Spark ML.\"\"\"\n",
    "    \n",
    "    # Generate test data with VALID correlation matrix\n",
    "    np.random.seed(42)\n",
    "    # Valid positive-semidefinite correlation matrix\n",
    "    corr_matrix = [[1.0, 0.6, 0.3], [0.6, 1.0, 0.4], [0.3, 0.4, 1.0]]\n",
    "    data = np.random.multivariate_normal([0, 0, 0], corr_matrix, n_rows)\n",
    "    \n",
    "    pdf_data = pd.DataFrame({\"col1\": data[:, 0], \"col2\": data[:, 1], \"col3\": data[:, 2]})\n",
    "    spark_df = spark.createDataFrame(pdf_data)\n",
    "    \n",
    "    # Benchmark pandas (what statsmodels uses internally)\n",
    "    pandas_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        corr = pdf_data.corr(method=\"spearman\").values\n",
    "        pandas_times.append(time.time() - start)\n",
    "    \n",
    "    # Benchmark Spark ML correlation ONLY (no distribution fitting)\n",
    "    spark_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        assembler = VectorAssembler(inputCols=[\"col1\", \"col2\", \"col3\"], outputCol=\"features\", handleInvalid=\"skip\")\n",
    "        vector_df = assembler.transform(spark_df).select(\"features\")\n",
    "        corr_result = Correlation.corr(vector_df, \"features\", method=\"spearman\")\n",
    "        _ = corr_result.head()[0].toArray()  # Force execution\n",
    "        spark_times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"pandas_ms\": np.mean(pandas_times) * 1000,\n",
    "        \"spark_ml_ms\": np.mean(spark_times) * 1000,\n",
    "    }\n",
    "\n",
    "print(\"Correlation-Only Benchmark (apples-to-apples)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'N Rows':>12} | {'pandas (ms)':>14} | {'Spark ML (ms)':>14} | {'Notes':>22}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for n in [10_000, 100_000, 1_000_000]:\n",
    "    result = benchmark_correlation_only(n, n_runs=2)\n",
    "    ratio = result[\"spark_ml_ms\"] / result[\"pandas_ms\"]\n",
    "    notes = f\"Spark {ratio:.0f}x slower (overhead)\"\n",
    "    print(f\"{result['n_rows']:>12,} | {result['pandas_ms']:>14.1f} | {result['spark_ml_ms']:>14.1f} | {notes:>22}\")\n",
    "\n",
    "print(\"\\n** Spark overhead is expected for local mode. The value is MEMORY scale, not speed. **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iwh6rzmn81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_sampling(n_samples: int, n_runs: int = 3):\n",
    "    \"\"\"Benchmark sample generation: statsmodels vs spark-bestfit.\"\"\"\n",
    "    \n",
    "    # Use the copula we already fitted (copula_3d from section 7)\n",
    "    corr_matrix_np = copula_3d.correlation_matrix\n",
    "    \n",
    "    # Create statsmodels copula with same correlation\n",
    "    sm_copula = StatsmodelsGaussianCopula(corr=corr_matrix_np, k_dim=3)\n",
    "    \n",
    "    # Benchmark statsmodels sampling (single-node, returns uniform only)\n",
    "    sm_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        sm_samples = sm_copula.rvs(n_samples)\n",
    "        sm_times.append(time.time() - start)\n",
    "    \n",
    "    # Benchmark spark-bestfit local sampling (includes marginal transform)\n",
    "    local_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        samples = copula_3d.sample(n=n_samples, random_state=42)\n",
    "        local_times.append(time.time() - start)\n",
    "    \n",
    "    # Benchmark spark-bestfit distributed sampling\n",
    "    spark_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        samples_df = copula_3d.sample_spark(n=n_samples, random_seed=42)\n",
    "        _ = samples_df.count()  # Force execution\n",
    "        spark_times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"statsmodels_ms\": np.mean(sm_times) * 1000,\n",
    "        \"local_ms\": np.mean(local_times) * 1000,\n",
    "        \"spark_ms\": np.mean(spark_times) * 1000,\n",
    "    }\n",
    "\n",
    "print(\"\\nSampling Benchmark\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'N Samples':>12} | {'statsmodels (ms)':>16} | {'local (ms)':>12} | {'spark (ms)':>12} | {'Fastest':>14}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for n in [1_000_000, 10_000_000, 50_000_000]:\n",
    "    result = benchmark_sampling(n, n_runs=2)\n",
    "    times = [(\"statsmodels\", result[\"statsmodels_ms\"]), \n",
    "             (\"local\", result[\"local_ms\"]), \n",
    "             (\"spark\", result[\"spark_ms\"])]\n",
    "    winner = min(times, key=lambda x: x[1])[0]\n",
    "    print(f\"{result['n_samples']:>12,} | {result['statsmodels_ms']:>16.1f} | {result['local_ms']:>12.1f} | {result['spark_ms']:>12.1f} | {winner:>14}\")\n",
    "\n",
    "print(\"\\n** Note: statsmodels returns uniform samples only; spark-bestfit includes marginal transform. **\")\n",
    "print(\"** At 100M+ samples, single-node approaches may exhaust memory - spark_spark() scales. **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j349sd10n5",
   "metadata": {},
   "source": [
    "### Benchmark Takeaways\n",
    "\n",
    "**Correlation Computation:**\n",
    "- pandas/statsmodels is **faster** for data that fits in memory (expected!)\n",
    "- Spark ML has overhead but can handle **billions of rows** that would crash pandas\n",
    "\n",
    "**Sampling:**\n",
    "- Local methods (statsmodels, `sample()`) are faster for small-medium samples\n",
    "- `sample_spark()` distributes work across the cluster for massive scale\n",
    "\n",
    "**When spark-bestfit copula makes sense:**\n",
    "1. **Data already in Spark** - avoid expensive `.toPandas()` to compute correlation\n",
    "2. **Data exceeds memory** - 100M+ rows won't fit in pandas, Spark is the only option\n",
    "3. **Massive sample generation** - 100M+ correlated samples for Monte Carlo simulation\n",
    "4. **Production pipelines** - integrate with existing Spark ETL workflows\n",
    "\n",
    "**When to use statsmodels instead:**\n",
    "- Data fits comfortably in memory (< 10M rows)\n",
    "- You're not already using Spark\n",
    "- You need fastest possible local performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `GaussianCopula` class enables correlated multi-column sampling **at scale**:\n",
    "\n",
    "| Scenario | statsmodels | spark-bestfit |\n",
    "|----------|-------------|---------------|\n",
    "| Data < 10M rows | **Faster** (use this) | Slower (Spark overhead) |\n",
    "| Data > 100M rows | Crashes (OOM) | **Works** (distributed) |\n",
    "| Data in Spark | Requires `.toPandas()` | **Native** (no conversion) |\n",
    "| 100M+ samples | May OOM | **`sample_spark()`** distributed |\n",
    "\n",
    "**Use spark-bestfit copula when:**\n",
    "- Data is already in a Spark DataFrame\n",
    "- Data exceeds single-node memory (100M+ rows)\n",
    "- You need 100M+ correlated samples for simulation\n",
    "- You're building production Spark pipelines\n",
    "\n",
    "**Key Methods:**\n",
    "- `GaussianCopula.fit(results, df)` - Fit copula from multi-column results\n",
    "- `copula.sample(n)` - Local sampling for small scale\n",
    "- `copula.sample_spark(n)` - Distributed sampling for massive scale\n",
    "- `copula.save(path)` / `GaussianCopula.load(path)` - Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
